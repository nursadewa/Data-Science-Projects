# -*- coding: utf-8 -*-
"""Sesi 24-Project_3_Agung_Nur_Sadewa.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1FlJjx1aF8JYPjgRd1D2T5s3gRIVjj91p

**Step Pengolahan Data**


1.   EDA
2.   Pre-Processing
3.   Modelling
4.   Model evaluation
etc
"""

# import library
import pandas as pd
import numpy as np

#import visualization library
import matplotlib.pyplot as plt
import seaborn as sns

#import ML Data Pre-Processing
from sklearn.model_selection import train_test_split
from sklearn.preprocessing import LabelEncoder, OneHotEncoder, MinMaxScaler, RobustScaler, StandardScaler

#import ML Model
from sklearn.linear_model import LinearRegression, Ridge, Lasso, LogisticRegression
from sklearn import metrics
from sklearn.metrics import r2_score, confusion_matrix, classification_report
from sklearn.neighbors import KNeighborsClassifier

"""# Load Dataset"""

data = pd.read_csv('https://raw.githubusercontent.com/tyoamazinglib/data_batch32/main/Dataset2_House_Price.csv')
data.head(10)

data.info()

data.dtypes

data.shape

data.isna().sum()

"""Tidak ada data yang kosong/null value"""

# cek data duplikat
dd = data[data.duplicated]
print('Number of duplicates:', dd.shape)

"""Tidak ada data yang duplikat

# EDA
"""

data.describe()

sns.set(style='whitegrid')
f, ax = plt.subplots(1,1, figsize=(8,8))
ax = sns.histplot(data['price'], kde=True, color='c')
plt.title='Distribusi Harga Rumah'

"""Hasil Histogram plot diatas menunjukkan Distribusi Normal(Bell Curve)"""

# Visualisasi Squaremeters dengan price --> Berdasarkan observasi awal keduanya memiliki relasi yang cukup tinggi
ax = sns.lmplot(x='squareMeters', y='price', hue='hasStorageRoom', data=data, palette= 'Set1')

"""Visualisasi dari plot diatas menunjukkan hubungan yang signifikan antara squareMeters dengan price, yang artinya semakin luas squareMeters maka harga dari rumah akan semakin mahal."""

# Visualisasi made dengan price --> Berdasarkan observasi awal tidak ada hubungan yang signifikan
ax = sns.lmplot(x='made', y='price', data=data, palette= 'Set1')

"""Visualisasi dari plot diatas tidak menunjukkan hubungan yang signifikan"""

# Visualisasi untuk cek apakah kolom squareMeters memiliki pengaruh dengan numberOfRooms
ax = sns.lmplot(x='numberOfRooms', y='squareMeters', data=data, palette= 'Set1')

"""Visualisasi dari plot diatas antara numberOfRooms dan squareMeters tidak menunjukkan hubungan yang signifikan"""

# Visualisasi untuk cek apakah kolom made memiliki pengaruh dengan numberOfRooms
ax = sns.lmplot(x='numberOfRooms', y='made', data=data, palette= 'Set1')

"""Visualisasi dari plot diatas antara made dan numberOfRooms tidak menunjukkan hubungan yang signifikan"""

sns.countplot(x='hasStorageRoom',data=data)

"""Visualisasi dari kolom hasStorageRoom, estimasi ada sekitar 51000 yang memiliki ruang gudang, dan sisanya tidak memiliki ruang gudang."""

sqMeters = data['squareMeters']
sns.boxplot(x=sqMeters)
plt.xlabel('Subject')
plt.ylabel('squareMeters Distribution')
plt.xticks([1], ['squareMeters'])
plt.show()

plt.scatter(x='numberofRooms',y='hasGuestRoom')

"""visualisasi dari scatterplot diatas, antara numberofRooms dan hasGuestRoom tidak ada pengaruh yang signifikan"""

#Correlation Matrix
f, ax = plt.subplots(1,1, figsize=(8,8))
ax = sns.heatmap(data.corr(), annot=True, cmap='cool')

# pairPlot
sns.pairplot(data, hue ='hasStorageRoom')
# to show
plt.show()

"""# Data Pre-processing"""

# Data sudah berbentuk int dan float (maka tidak perlu encoding)
dfc = data.copy()

# Lakukan scaling (jika perlu) --> Scaling diperlukan ketika range antara kolom sangat jauh
# Standar scaling
scaler = StandardScaler()

#FitTransform untuk melakukan standar scaling
scaled_data = scaler.fit_transform(dfc)

#Konversi scaling kedalam DataFrame
stdr_data = pd.DataFrame(scaled_data, columns=dfc.columns)

print('DataFrame setelah scaling:')
print(stdr_data)

#MinMax Scaling
scaler2 = MinMaxScaler(feature_range=(0,1))

#Robust Scaling
scaler3 = RobustScaler()

# Train test split_data
x = stdr_data.drop(['price'], axis=1) # prediktor
y = stdr_data['price'] #Label
x_train, x_test, y_train, y_test = train_test_split(x,y, test_size = 0.2, random_state = 42)

"""# Modelling

## Training and Testing Model
Komparasi antara training dan testing score
"""

x_train_scaled = scaler.fit_transform(x_train)
x_test_scaled = scaler.transform(x_test)

x_train_scaled = pd.DataFrame(x_train_scaled, columns=x.columns)
x_test_scaled = pd.DataFrame(x_test_scaled, columns=x.columns)

x_train_scaled.head()

"""# **Classification Model**

### **Function Evaluation Metrics**
"""

# Evaluation Metrics
def classification_eval(aktual, prediksi, name):
  cm = confusion_matrix(aktual, prediksi)
  tp = cm[1][1]
  tn = cm[0][0]
  fp = cm[0][1]
  fn = cm[1][0]
  print(tp, tn, fp, fn)

  accuracy = round((tp+tn) / (tp+tn+fp+fn) * 100, 2)
  precision =  round((tp) / (tp+fp) * 100, 2)
  recall = round((tp) / (tp+fn) * 100, 2)

  print('Evaluation Model:', name)
  print(cm)
  print('Accuracy :', accuracy, '%')
  print('Precison :', precision, '%')
  print('Recall   :', recall, '%')

"""###**Linear Regression**"""

# Linear
Lin_reg = LinearRegression()
Lin_reg.fit(x_train, y_train)
y_train_pred = Lin_reg.predict(x_train)
y_test_pred = Lin_reg.predict(x_test)
print('Score:', Lin_reg.score(x_train, y_train))
print('Score:', Lin_reg.score(x_test, y_test))

"""###**Ridge Regression**"""

# Ridge
ridge_model = Ridge()
ridge_model.fit(x_train, y_train)
y_train_pred = ridge_model.predict(x_train)
y_test_pred = ridge_model.predict(x_test)
print('Score:', ridge_model.score(x_train, y_train))
print('Score:', ridge_model.score(x_test, y_test))

"""###**Lasso Regression**"""

# Lasso
Lasso = Lasso(alpha=0.2, fit_intercept=True, precompute=False, max_iter=1000,
              tol=0.0001, warm_start=False, positive=False, random_state=None, selection='cyclic')
Lasso.fit(x_train, y_train)

print('Score:', Lasso.score(x_train, y_train))
print('Score:', Lasso.score(x_test, y_test))

"""###**Logistic Regression**"""

# Logistic Regression
klasifikasi = np.linspace(min(y), max(y), num=6) # sesuaikan nilai number untuk mendapatkan nilai score test maximal
y_class = np.digitize(y, klasifikasi) - 1

# y_class digunakan sebagai variable untuk type classification
x_train, x_test, y_train, y_test = train_test_split(x, y_class, test_size=0.2, random_state=42)

Log_reg = LogisticRegression()
Log_reg.fit(x_train, y_train)
y_train_pred = Log_reg.predict(x_train)
y_test_pred = Log_reg.predict(x_test)
Log_reg.score(x_test, y_test)

print('Score:', Log_reg.score(x_train, y_train))
print('Score:', Log_reg.score(x_test, y_test))

"""###**KNN with MaxMinScaler**"""

# KNN
x_train = scaler2.fit_transform(x_train)
x_test = scaler2.fit_transform(x_test)
knn = KNeighborsClassifier(n_neighbors=9) # n_neighbors=9, Mendapatkan score test terbaik
knn.fit(x_train, y_train)
y_pred = knn.predict(x_test)
print(y_pred)
knn.score(x_test, y_test)
cm = confusion_matrix(y_test, y_pred)
print(cm)

cr = classification_report(y_test, y_pred)
print(cr)
print(knn.n_samples_fit_)

print('Score:', knn.score(x_train, y_train))
print('Score:', knn.score(x_test, y_test))

"""###**KNN with StandarScaler**"""

# KNN Standar Scaler
knn = KNeighborsClassifier(n_neighbors=9)
knn.fit(x_train_scaled, y_train)

y_train_pred = knn.predict(x_train_scaled)
y_test_pred = knn.predict(x_test_scaled)

classification_eval(y_train, y_train_pred, 'KNN Training')

classification_eval(y_test, y_test_pred, 'KNN Test')

"""# **Evaluating Model**"""

# Model Improvement and Tunning
Lin_reg.fit(x_train, y_train)
score_with_no_scaling = Lin_reg.score(x_test, y_test)
print("Accuracy - Before Scaled:", score_with_no_scaling)

Lin_reg.fit(x_train_scaled, y_train)
score_with_scaling = Lin_reg.score(x_test_scaled, y_test)
print("Accuracy - After Scaled:", score_with_scaling)

pctg = round((score_with_scaling - score_with_no_scaling)/score_with_no_scaling, 5)
print(f'Score Improvement: {pctg*100}%')

"""# Model Tuning"""

# Model Tuning
from sklearn.model_selection import GridSearchCV
param_grid = {'alpha':[0.1, 1, 10, 100], 'solver':['auto', 'svd', 'saga', 'lsqr', 'cholesky'] }

grid_search = GridSearchCV(ridge_model, param_grid, cv=5, scoring='neg_mean_squared_error')
grid_search.fit(x_train, y_train)

print('Parameter Terbaik:', grid_search.best_params_)

#prediksi
y_pred = grid_search.predict(x_test)

print('R2 Score', r2_score(y_test, y_pred))

ridge_model = Ridge(alpha=0.1, solver='lsqr')
ridge_model.fit(x_train, y_train)
print('Score', ridge_model.score(x_test, y_test))

Lin_reg = LinearRegression()
Lin_reg.fit(x_train, y_train)
print('Score:', Lin_reg.score(x_test, y_test))

"""### Berdasarkan hasil di atas, kita akan mengambil model secara berurutan:
1. Linear Regression
2. Ridge Model
3. Logistic Regression
4. Lasso Regression
5. KNN Regressor

Karena Score Testing nya lebih tinggi dari nilai score test yang lainnya.
"""

import pickle
pickle.dump(ridge_model, open("Ridge_model.pkl","wb"))

"""# **Mengintegrasi Model kedalam Flask**"""

# Deployment
!pip install pyngrok

!pip install flask

# pake Flask untuk deploy model
from flask import Flask
from flask import jsonify, request
from pyngrok import ngrok
from datetime import datetime

app = Flask(__name__)

# Load Model ke dalam API
with open('Ridge_model.pkl','rb') as file:
  model = pickle.load(file)

# Route menggunakan model, metode POST
@app.route('/predict', methods=['POST'])
def predict():
  # Ambil JSON data request dari POSTMAN untuk API
  datax = request.get_json()

  # Ekstrak nilai kolom data dari json
  made = datax["made"]
  squareMeters = datax["squareMeters"]
  numberOfRooms = datax["numberOfRooms"]
  hasStorageRoom = datax["hasStorageRoom"]
  hasGuestRoom = datax["hasGuestRoom"]

  # Melakukan prediksi
  prediction = model.predict([[made, squareMeters, numberOfRooms, hasStorageRoom, hasGuestRoom]])
  datax['Prediction'] = prediction[0]

  with open('data_collection.txt', 'a') as file2:
    file2.write("%s\n" %datax)

  return jsonify({"Status":"Sukses", "Prediction": str(prediction[0])})

if __name__== "__main__":
  ngrok.set_auth_token("2iMTdhWRZxqMkrpkPJPHdhsi7uO_RSXxAiHWjH8QWuMwEbi5")
  ngrok_tunnel = ngrok.connect(5000)
  print("Public URL: ", ngrok_tunnel.public_url)
  app.run()

"""# Kesimpulan

Dari data diatas dapat disimpulkan bahwa:


1.   Data tersebut hanya menjelaskan **harga rumah di satu lokasi** pada waktu tertentu berdasarkan ukuran bangunan, tanpa melihat kapan pembuatan rumah yang menentukan depresiasinya dan jumlah ruangan yang dimilikinya.

      Jika ingin melihat harga lebih tepat dan memiliki korelasi antara jumlah ruangan dan harga, kurang lengkap karena:

     ◘ Tidak ada ukuran per ruangan, tinggi bangunan, apakah bangunan dengan 1 lantai atau beberapa lantai ke atas, apakah rumah tinggal/sewa, .

     ◘ Jika ini ada rumah dengan beberapa lantai, maka bisa dipastikan ada ruangan kecil didalam ruangan besar, walaupun nilai NumberofRooms kecil bisa dipastikan ukuran ruangan/kamar itu besar dengan beberapa ruangan/kamar/kecil yang didalamnya tersedia guestroom/ruang tamu kecil.

     ◘ Jika hanya satu lantai, berarti hanya terdapat ruangan kecil dengan didalamnya ada beberapa yang memiliki guestofRoom.


2.   Model yang paling tepat adalah Model Linear Regression

3.   Untuk perbaikan maka data ini harus dilakukan: Jika ingin membuat data lebih bermakna sehingga kolom-kolom yang tersedia memberikan manfaat dan untuk memprediksi price lebih akurat karena ada korelasi antar kolom, maka:

      ○ penambahan kolom ukuran ruangan terutama untuk size(numberofRooms)

      ○ penambahan kolom NumberofFloors, jika berbentuk bangunan tinggi., seperti contohnya Apartemen atau Rumah Susun.

      **Tambahan:**

      ○ Penambahan kolom keterangan kelas ruangan, jika rumah tersebut rumah sewa yang memiliki kelas yang menentukan ukuran, dan yang menentukan memiliki ruang guestofRoom atau tidak.

(Optional) Bisa menambahkan Model Deployment 1 dengan Flask untuk extra score.
"""